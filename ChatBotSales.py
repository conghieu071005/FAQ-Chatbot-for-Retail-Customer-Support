
import os, re, sys, math, json, unicodedata, shutil
from typing import List, Dict, Tuple
import chromadb
from chromadb.config import Settings

# ================== C·∫§U H√åNH ==================
DEFAULT_JSON_PATH = r"C:\VS_code\Data2.json"
PERSIST_DIR = r"C:\Users\nguye\OneDrive\Desktop\Data2"
COLLECTION_NAME = "faq_tfidf_vi"

# B·∫≠t/t·∫Øt c√°c tu·ª≥ ch·ªçn c·∫£i thi·ªán
ACCENT_INSENSITIVE = True          # so kh·ªõp kh√¥ng d·∫•u cho truy v·∫•n & c√¢u h·ªèi
USE_STOPWORDS = True               # b·ªè t·ª´ d·ª´ng
ALPHA = 0.7                        # tr·ªçng s·ªë cosine (0..1). 0.7 = ∆∞u ti√™n cosine, 0.3 = overlap
TOP_K = 3                          # tr·∫£ v·ªÅ n·ªôi b·ªô, ch·ªçn best trong top-K
MIN_SCORE = 0.12                   # ng∆∞·ª°ng t·ªëi thi·ªÉu; d∆∞·ªõi ng∆∞·ª°ng -> t·ª´ ch·ªëi ƒëo√°n b·ª´a
BATCH_SIZE = 100                   # <= 166 ƒë·ªÉ tr√°nh l·ªói batch c·ªßa Chroma

# Stopwords ti·∫øng Vi·ªát g·ªçn (c√≥ th·ªÉ m·ªü r·ªông)
VI_STOPWORDS = set("""
l√† th√¨ m√† v√† v·ªõi ho·∫∑c nh∆∞ng c·ªßa c√°c nh·ªØng c√°i m·ªôt m·ªôt s·ªë cho v√†o ra l√™n xu·ªëng t·∫°i t·ª´ ƒë·∫øn ƒëang s·∫Ω ƒë√£ ƒë∆∞·ª£c ch∆∞a c≈©ng n·ªØa
r·∫±ng n·∫øu khi v√¨ b·ªüi n√™n nh∆∞ v·∫≠y v.v v.v. ·∫° ·∫°? ·∫°! √† ·ª´ ·ªù nh√© nha nh√° h·∫£ kh√¥ng ko k hok v·∫≠y th√¥i ƒëi ha
""".split())

# ƒê·ªìng nghƒ©a/chu·∫©n ho√° √Ω (th√™m _ ƒë·ªÉ gi·ªØ token ƒë∆°n)
SYNONYMS = {
    "b√°n": "m·∫∑t_h√†ng",
    "b√°n_g√¨": "m·∫∑t_h√†ng",
    "b·∫°n_b√°n_g√¨": "m·∫∑t_h√†ng",
    "m·∫∑t": "m·∫∑t_h√†ng",
    "m·∫∑t_h√†ng": "m·∫∑t_h√†ng",
    "kinh_doanh": "m·∫∑t_h√†ng",
    "shop_b√°n_g√¨": "m·∫∑t_h√†ng",
    "c√≥_b√°n": "m·∫∑t_h√†ng",
    "c√≥_mi·ªÖn_ph√≠": "mi·ªÖn_ph√≠",
    "free": "mi·ªÖn_ph√≠",
    "freeship": "mi·ªÖn_ph√≠",
}

PROFANITY = {"ƒëm","ƒë·ªãt","c·∫∑c","l·ªìn","ƒë·ª•","ƒë√©o","m·∫π","b·ªë m√†y","vkl","vcl"}

# ================== TI·ªÜN √çCH CHU·∫®N HO√Å ==================
_word_re = re.compile(r"[a-zA-Z√Ä-·ªπ0-9_]+", flags=re.UNICODE)

def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return unicodedata.normalize("NFC", s)

def normalize_text(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFC", s).lower()
    return strip_accents(s) if ACCENT_INSENSITIVE else s

def tokenize_raw(s: str) -> List[str]:
    # token ƒë∆°n gi·∫£n + n·ªëi t·ª´ gh√©p ph·ªï bi·∫øn
    s = normalize_text(s)
    s = s.replace("b√°n g√¨", "b√°n_g√¨").replace("m·∫∑t h√†ng", "m·∫∑t_h√†ng").replace("kinh doanh", "kinh_doanh").replace("c√≥ mi·ªÖn ph√≠","c√≥_mi·ªÖn_ph√≠")
    toks = _word_re.findall(s)
    # map synonyms
    mapped = [SYNONYMS.get(t, t) for t in toks]
    if USE_STOPWORDS:
        mapped = [t for t in mapped if t not in VI_STOPWORDS]
    return mapped

def has_profanity(s: str) -> bool:
    s2 = normalize_text(s)
    return any(bad in s2 for bad in PROFANITY)

# ================== TF-IDF T·ª∞ C√ÄI ƒê·∫∂T ==================
class TfidfVectorizerManual:
    def __init__(self):
        self.vocab: Dict[str,int] = {}
        self.idf: List[float] = []
        self.fitted = False
        self.doc_tokens: List[List[str]] = []  # gi·ªØ l·∫°i cho overlap

    def fit(self, docs: List[str]):
        docs_tokens = [tokenize_raw(d) for d in docs]
        self.doc_tokens = docs_tokens
        vocab = {}
        for toks in docs_tokens:
            for t in toks:
                if t not in vocab:
                    vocab[t] = len(vocab)
        self.vocab = vocab
        N = len(docs_tokens)
        df = [0]*len(vocab)
        for toks in docs_tokens:
            seen = set()
            for t in toks:
                idx = vocab.get(t)
                if idx is not None and idx not in seen:
                    df[idx]+=1
                    seen.add(idx)
        # Smooth IDF
        self.idf = [math.log((1+N)/(1+dfi))+1.0 for dfi in df]
        self.fitted = True

    def transform_one(self, doc: str) -> List[float]:
        assert self.fitted
        toks = tokenize_raw(doc)
        if not toks: return [0.0]*len(self.vocab)
        counts: Dict[int,int] = {}
        for t in toks:
            idx = self.vocab.get(t)
            if idx is not None:
                counts[idx] = counts.get(idx,0)+1
        tfidf = [0.0]*len(self.vocab)
        L = float(len(toks))
        for idx,c in counts.items():
            tfidf[idx] = (c/L)*self.idf[idx]
        return tfidf

    def transform(self, docs: List[str]) -> List[List[float]]:
        return [self.transform_one(d) for d in docs]

# ================== SIMILARITY ==================
def cosine_sim(a: List[float], b: List[float]) -> float:
    dot = na = nb = 0.0
    for x,y in zip(a,b):
        dot += x*y; na += x*x; nb += y*y
    if na==0 or nb==0: return 0.0
    return dot/(math.sqrt(na)*math.sqrt(nb))

def token_overlap_score(q_tokens: List[str], d_tokens: List[str]) -> float:
    if not q_tokens or not d_tokens: return 0.0
    qs, ds = set(q_tokens), set(d_tokens)
    inter = len(qs & ds)
    denom = min(len(qs), len(ds))
    return inter/denom if denom>0 else 0.0

# ================== CHROMADB QU·∫¢N L√ù ==================
def get_collection():
    client = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(allow_reset=False))
    try:
        col = client.get_collection(COLLECTION_NAME)
    except Exception:
        col = client.create_collection(COLLECTION_NAME)
    return col

def reset_chroma_if_corrupted():
    try:
        _ = get_collection()
    except Exception:
        if os.path.exists(PERSIST_DIR):
            shutil.rmtree(PERSIST_DIR, ignore_errors=True)
        os.makedirs(PERSIST_DIR, exist_ok=True)
        _ = get_collection()

def load_faq_json(path: str) -> List[Dict[str,str]]:
    with open(path,"r",encoding="utf-8") as f:
        data = json.load(f)
    out=[]
    for i,item in enumerate(data):
        q = (item.get("question") or "").strip()
        a = (item.get("answer") or "").strip()
        if q and a:
            out.append({"id": f"faq_{i}", "question": q, "answer": a})
    if not out:
        raise ValueError("JSON kh√¥ng c√≥ m·ª•c question/answer h·ª£p l·ªá.")
    return out

def build_or_refresh_index(faq_items: List[Dict[str,str]], vectorizer: TfidfVectorizerManual|None=None) -> TfidfVectorizerManual:
    reset_chroma_if_corrupted()
    col = get_collection()

    questions = [x["question"] for x in faq_items]
    if vectorizer is None:
        vectorizer = TfidfVectorizerManual()
        vectorizer.fit(questions)
    vectors = vectorizer.transform(questions)

    # Xo√° d·ªØ li·ªáu c≈© (n·∫øu c√≥)
    try:
        existing = col.get(include=["embeddings","metadatas","documents"])
        if existing.get("ids"):
            col.delete(ids=existing["ids"])
    except Exception:
        pass

    # Th√™m theo batch
    ids = [x["id"] for x in faq_items]
    metadatas = [{"answer": x["answer"], "question": x["question"]} for x in faq_items]
    for i in range(0, len(faq_items), BATCH_SIZE):
        end = i + BATCH_SIZE
        col.add(
            ids=ids[i:end],
            documents=questions[i:end],
            metadatas=metadatas[i:end],
            embeddings=vectors[i:end],
        )
    print(f"‚úÖ ƒê√£ n·∫°p {len(faq_items)} c√¢u h·ªèi v√†o ChromaDB (chia {math.ceil(len(faq_items)/BATCH_SIZE)} batch).")
    return vectorizer

def ensure_vectorizer(faq_items: List[Dict[str,str]]):
    return build_or_refresh_index(faq_items)

# ================== TRUY V·∫§N ==================
def hybrid_score(query: str, doc_index: int, cosine_val: float, vectorizer: TfidfVectorizerManual) -> float:
    # Overlap gi·ªØa query v√† c√¢u h·ªèi g·ªëc t∆∞∆°ng ·ª©ng
    q_tokens = tokenize_raw(query)
    d_tokens = vectorizer.doc_tokens[doc_index] if 0 <= doc_index < len(vectorizer.doc_tokens) else []
    ov = token_overlap_score(q_tokens, d_tokens)
    return ALPHA * cosine_val + (1.0 - ALPHA) * ov

def answer_question(user_q: str, vectorizer: TfidfVectorizerManual) -> Tuple[str, float, Dict]:
    if has_profanity(user_q):
        return "M√¨nh kh√¥ng th·ªÉ h·ªó tr·ª£ v·ªõi n·ªôi dung kh√¥ng ph√π h·ª£p. B·∫°n vui l√≤ng h·ªèi l·∫°i theo c√°ch kh√°c nh√©.", 0.0, {}

    col = get_collection()
    store = col.get(include=["embeddings","metadatas","documents"])
    if not store["ids"]:
        return "Ch∆∞a c√≥ d·ªØ li·ªáu FAQ.", 0.0, {}

    q_vec = vectorizer.transform_one(user_q)

    # T√≠nh cosine cho t·∫•t c·∫£
    scores = []
    for i, emb in enumerate(store["embeddings"]):
        cos = cosine_sim(q_vec, emb)
        scores.append((i, cos))

    # L·∫•y top-K theo cosine r·ªìi t√≠nh hybrid
    scores.sort(key=lambda x: x[1], reverse=True)
    top = scores[:TOP_K]
    best_idx, best_hscore, best_cos = -1, -1.0, 0.0
    for i, cos in top:
        h = hybrid_score(user_q, i, cos, vectorizer)
        if h > best_hscore:
            best_hscore, best_idx, best_cos = h, i, cos

    if best_hscore < MIN_SCORE:
        return "M√¨nh ch∆∞a ch·∫Øc b·∫°n c·∫ßn g√¨. B·∫°n m√¥ t·∫£ r√µ h∆°n (vd: 'c√≥ mi·ªÖn ph√≠ v·∫≠n chuy·ªÉn n·ªôi th√†nh kh√¥ng?') nh√©.", best_hscore, {}

    meta = {
        "matched_question": store["documents"][best_idx],
        "answer": store["metadatas"][best_idx]["answer"],
        "cosine": round(float(best_cos), 6),
        "score": round(float(best_hscore), 6),
    }
    return meta["answer"], best_hscore, meta

# ================== MAIN ==================
def main():
    json_path = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_JSON_PATH
    if not os.path.exists(json_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file JSON: {json_path}")
        return
    faq_items = load_faq_json(json_path)
    vectorizer = ensure_vectorizer(faq_items)

    print("=== ElectroStore FAQ Chatbot (TF-IDF + ChromaDB + Hybrid) ===")
    print("G√µ c√¢u h·ªèi (ti·∫øng Vi·ªát). G√µ 'exit' ƒë·ªÉ tho√°t.\n")

    while True:
        try:
            q = input("B·∫°n: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nT·∫°m bi·ªát!")
            break
        if q.lower() in {"exit","quit"}:
            print("T·∫°m bi·ªát!")
            break
        if not q:
            continue

        ans, score, meta = answer_question(q, vectorizer)
        print(f"Bot: {ans}")
        if meta:
            print(f"(Kh·ªõp: \"{meta.get('matched_question','')}\" | cosine={meta.get('cosine',0.0)} | score={meta.get('score',0.0)})\n")
        else:
            print(f"(score={round(score,6)})\n")

# ================== GIAO DI·ªÜN STREAMLIT ==================
import streamlit as st
import os

# Gi·∫£ s·ª≠ b·∫°n c√≥ c√°c h√†m sau (t√πy b·∫°n ƒë·ªãnh nghƒ©a ·ªü n∆°i kh√°c)
# from your_module import load_faq_json, ensure_vectorizer, answer_question, DEFAULT_JSON_PATH

def run_streamlit_ui():
    st.set_page_config(
        page_title="ElectroStore Chatbot",
        page_icon="‚ö°",
        layout="centered"
    )

    # ========== SIDEBAR ==========
    with st.sidebar:
        st.markdown("## ‚ö° **CellPhoneS**")
        st.markdown("""
        **ƒê·ªãa ch·ªâ:** 71 Ng≈© H√†nh S∆°n, P. M·ªπ An, Q. Ng≈© H√†nh S∆°n, ƒê√† N·∫µng  
        **ƒêi·ªán tho·∫°i:** 0868357896    
        **Email:** Nguyenconghieu071005@gmail.com  
        **Gi·ªù l√†m vi·ªác:** 8:00 - 21:00 (T2 - CN)  
        """)
        st.markdown("üí¨ *Chatbot h·ªó tr·ª£ t∆∞ v·∫•n s·∫£n ph·∫©m v√† ch√≠nh s√°ch c·ª≠a h√†ng*")

    # ========== MAIN CONTENT ==========
    st.title("Ch√†o b·∫°n ƒë·∫øn v·ªõi **CellPhoneS**!")
    st.caption("H·ªèi v·ªÅ s·∫£n ph·∫©m Apple, ch√≠nh s√°ch b·∫£o h√†nh, tr·∫£ g√≥p...")

    # Load d·ªØ li·ªáu & vectorizer
    if "vectorizer" not in st.session_state:
        if not os.path.exists(DEFAULT_JSON_PATH):
            st.error(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {DEFAULT_JSON_PATH}")
            return

        faq_items = load_faq_json(DEFAULT_JSON_PATH)
        vectorizer = ensure_vectorizer(faq_items)
        st.session_state.vectorizer = vectorizer
        st.session_state.faq_items = faq_items
        st.session_state.history = []

    vectorizer = st.session_state.vectorizer

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for item in st.session_state.history:
        with st.chat_message("user"):
            st.markdown(item["q"])
        with st.chat_message("assistant"):
            st.markdown(item["a"])


    # √î nh·∫≠p tin nh·∫Øn
    user_q = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (g√µ 'exit' ƒë·ªÉ tho√°t)...")

    if user_q:
        if user_q.lower().strip() in {"exit", "quit"}:
            st.stop()

        with st.chat_message("user"):
            st.markdown(user_q)

        ans, score, meta = answer_question(user_q, vectorizer)

        with st.chat_message("assistant"):
            st.markdown(ans)

        # L∆∞u l·ªãch s·ª≠ chat
        st.session_state.history.append({"q": user_q, "a": ans, "meta": meta})

if __name__ == "__main__":
    run_streamlit_ui()
